{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../data/songda/eval/alpaca_7B_with_semantics/truthful_qa/31/hidden_states_KMeans_15_5_0.2.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 71\u001b[0m\n\u001b[1;32m     65\u001b[0m abs_args \u001b[38;5;241m=\u001b[39m SimpleNamespace(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstate_abstract_args)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# abstractStateExtraction = AbstractStateExtraction(\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#     abs_args, None, None, None\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m hmm_model \u001b[38;5;241m=\u001b[39m \u001b[43mHmmModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextract_block_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfo_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcluster_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabstract_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpca_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_ratio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhmm_components_num\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miter_num\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_attack_success\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrid_history_dependency_num\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_abstract_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresult_eval_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m (\n\u001b[1;32m     86\u001b[0m     dtmc_transition_aucroc,\n\u001b[1;32m     87\u001b[0m     dtmc_transition_fpr,\n\u001b[1;32m     88\u001b[0m     dtmc_transition_tpr,\n\u001b[1;32m     89\u001b[0m ) \u001b[38;5;241m=\u001b[39m hmm_model\u001b[38;5;241m.\u001b[39mget_aucroc_by_transition_binding()\n\u001b[1;32m     91\u001b[0m prob_model \u001b[38;5;241m=\u001b[39m hmm_model\n",
      "File \u001b[0;32m~/LUNA/luna/probabilistic_abstraction_model.py:629\u001b[0m, in \u001b[0;36mHmmModel.__init__\u001b[0;34m(self, dataset, extract_block_idx, info_type, cluster_method, abstract_state, pca_dim, test_ratio, hmm_components_num, iter_num, is_attack_success, grid_history_dependency_num, result_eval_path)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    591\u001b[0m     dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    602\u001b[0m     result_eval_path,\n\u001b[1;32m    603\u001b[0m ):\n\u001b[1;32m    604\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m    Initialize the HmmModel class.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m        - It constructs the path to the evaluation folder based on the dataset and other parameters.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextract_block_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcluster_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mabstract_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpca_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_attack_success\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrid_history_dependency_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult_eval_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madvglue++\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cluster_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrid\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/LUNA/luna/probabilistic_abstraction_model.py:112\u001b[0m, in \u001b[0;36mProbabilisticModel.__init__\u001b[0;34m(self, dataset, extract_block_idx, info_type, cluster_method, abstract_state, pca_dim, test_ratio, is_attack_success, grid_history_dependency_num, result_eval_path)\u001b[0m\n\u001b[1;32m    110\u001b[0m eval_folder_path \u001b[38;5;241m=\u001b[39m result_eval_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m eval_folder_path\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_folder_path \u001b[38;5;241m=\u001b[39m eval_folder_path\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_instances, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_instances, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_instances \u001b[38;5;241m=\u001b[39m \u001b[43mload_lists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_folder_path\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_transition_matrix, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_transition_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_traces \u001b[38;5;241m=\u001b[39m [i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_instances]\n",
      "File \u001b[0;32m~/LUNA/luna/utils/interfaces.py:50\u001b[0m, in \u001b[0;36mload_lists\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lists\u001b[39m(filename):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     51\u001b[0m         list1, list2, list3 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m list1, list2, list3\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/songda/eval/alpaca_7B_with_semantics/truthful_qa/31/hidden_states_KMeans_15_5_0.2.pkl'"
     ]
    }
   ],
   "source": [
    "from luna.state_abstraction_utils import AbstractStateExtraction\n",
    "from luna.probabilistic_abstraction_model import (\n",
    "    HmmModel,\n",
    "    DtmcModel,\n",
    ")\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "\n",
    "llm = \"alpaca_7B_with_semantics\"\n",
    "dataset = \"truthful_qa\"\n",
    "result_save_path = \"../../../data/songda\"\n",
    "extract_block_idx = 31\n",
    "info_type = \"hidden_states\"\n",
    "abstraction_method = \"KMeans\"\n",
    "model_type = \"HMM\"\n",
    "hmm_n_comp = 100\n",
    "abstract_state_num = 15\n",
    "pca_dim = 5\n",
    "grid_history_dependency_num = 1\n",
    "\n",
    "state_abstract_args = {\n",
    "    \"llm_name\": llm,\n",
    "    \"result_save_path\": result_save_path,\n",
    "    \"dataset\": dataset,\n",
    "    \"test_ratio\": 0.2,\n",
    "    \"extract_block_idx\": extract_block_idx,\n",
    "    \"info_type\": info_type,\n",
    "    \"is_attack_success\": 1,\n",
    "    \"cluster_method\": abstraction_method,\n",
    "    \"abstract_state\": abstract_state_num,\n",
    "    \"pca_dim\": pca_dim,\n",
    "    \"grid_history_dependency_num\": grid_history_dependency_num,\n",
    "    \"result_eval_path\": \"{}/eval/{}\".format(result_save_path, llm),\n",
    "}\n",
    "\n",
    "\n",
    "prob_args = {\n",
    "    \"llm_name\": llm,\n",
    "    \"result_save_path\": result_save_path,\n",
    "    \"dataset\": dataset,\n",
    "    \"test_ratio\": 0.2,\n",
    "    \"extract_block_idx\": extract_block_idx,\n",
    "    \"info_type\": info_type,\n",
    "    \"is_attack_success\": 1,\n",
    "    \"iter_num\": 30,\n",
    "    \"cluster_method\": abstraction_method,\n",
    "    \"abstract_state\": abstract_state_num,\n",
    "    \"pca_dim\": pca_dim,\n",
    "    \"model_type\": model_type,\n",
    "    \"hmm_components_num\": hmm_n_comp if hmm_n_comp else \"\",\n",
    "    \"grid_history_dependency_num\": grid_history_dependency_num\n",
    "    if grid_history_dependency_num\n",
    "    else \"\",\n",
    "}\n",
    "\n",
    "stat_dict = {\n",
    "    \"proper_stopped_and_true\": 0,\n",
    "    \"proper_stopped_and_false\": 0,\n",
    "    \"loop_generated_and_true\": 0,\n",
    "    \"loop_generated_and_false\": 0,\n",
    "}\n",
    "\n",
    "abs_args = SimpleNamespace(**state_abstract_args)\n",
    "\n",
    "# abstractStateExtraction = AbstractStateExtraction(\n",
    "#     abs_args, None, None, None\n",
    "# )\n",
    "\n",
    "hmm_model = HmmModel(\n",
    "    prob_args[\"dataset\"],\n",
    "    prob_args[\"extract_block_idx\"],\n",
    "    prob_args[\"info_type\"],\n",
    "    prob_args[\"cluster_method\"],\n",
    "    prob_args[\"abstract_state\"],\n",
    "    prob_args[\"pca_dim\"],\n",
    "    prob_args[\"test_ratio\"],\n",
    "    prob_args[\"hmm_components_num\"],\n",
    "    prob_args[\"iter_num\"],\n",
    "    prob_args[\"is_attack_success\"],\n",
    "    prob_args[\"grid_history_dependency_num\"],\n",
    "    state_abstract_args[\"result_eval_path\"],\n",
    ")\n",
    "(\n",
    "    dtmc_transition_aucroc,\n",
    "    dtmc_transition_fpr,\n",
    "    dtmc_transition_tpr,\n",
    ") = hmm_model.get_aucroc_by_transition_binding()\n",
    "\n",
    "prob_model = hmm_model\n",
    "test_abstract_traces = hmm_model.test_traces\n",
    "val_abstract_traces = hmm_model.val_traces\n",
    "train_abstract_traces = hmm_model.train_traces\n",
    "\n",
    "train_data_points = [{} for _ in range(len(train_abstract_traces))]\n",
    "for i, one_trace in enumerate(train_abstract_traces):\n",
    "    train_data_points[i][\"step_by_step_analyzed_trace\"] = one_trace\n",
    "    train_data_points[i][\"label\"] = hmm_model.train_groundtruths[i]\n",
    "\n",
    "\n",
    "hmm_model.get_aucroc_by_state_binding()\n",
    "\n",
    "all_instances = (\n",
    "    hmm_model.train_instances + hmm_model.val_instances + hmm_model.test_instances\n",
    ")\n",
    "for instance in all_instances:\n",
    "    classification = instance[\"is_loop_generated\"]\n",
    "    if classification == 0:\n",
    "        if instance[\"binary_label\"] >= 0.5:\n",
    "            stat_dict[\"proper_stopped_and_true\"] += 1\n",
    "        else:\n",
    "            stat_dict[\"proper_stopped_and_false\"] += 1\n",
    "    else:\n",
    "        if instance[\"binary_label\"] >= 0.5:\n",
    "            stat_dict[\"loop_generated_and_true\"] += 1\n",
    "        else:\n",
    "            stat_dict[\"loop_generated_and_false\"] += 1\n",
    "\n",
    "# calculate stat_dict percentage\n",
    "total = sum(stat_dict.values())\n",
    "for key in stat_dict:\n",
    "    stat_dict[key] /= total\n",
    "\n",
    "train_transition_matrix = hmm_model.train_transition_probs\n",
    "semantic_value_dict = hmm_model.get_semantic_state_model()\n",
    "\n",
    "print(\"train_transition_matrix\")\n",
    "print(train_transition_matrix)\n",
    "print(\"state_semantics\")\n",
    "print(semantic_value_dict)\n",
    "# save matrix\n",
    "np.save(\n",
    "    \"train_transition_matrix.npy\",\n",
    "    train_transition_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{12: 0.8333333333333334, 274: 0.06060606060606061, 297: 0.007575757575757576, 136: 0.045454545454545456, 7: 0.007575757575757576, 202: 0.015151515151515152, 148: 0.007575757575757576, 20: 0.007575757575757576, 228: 0.007575757575757576, 242: 0.007575757575757576}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'eval/prism/truthful_qa_alpaca_7B_with_semantics_HMM_400_500.pm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruthful_qa\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     88\u001b[0m     train_state_matrix \u001b[38;5;241m=\u001b[39m prob_model\u001b[38;5;241m.\u001b[39mstate_positive_prob_map\n\u001b[0;32m---> 89\u001b[0m     \u001b[43mdtmc_to_prism_updated\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_transition_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43msemantic_value_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupdated_output_prism_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43msemantic_dataset_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     dtmc_to_prism_updated(\n\u001b[1;32m     99\u001b[0m         train_transition_matrix,\n\u001b[1;32m    100\u001b[0m         semantic_value_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m         semantic_dataset_dict,\n\u001b[1;32m    105\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[3], line 77\u001b[0m, in \u001b[0;36mdtmc_to_prism_updated\u001b[0;34m(dtmc_dict, semantic_value_dict, output_file_path, llm_name, dataset, semantic_dataset_dict)\u001b[0m\n\u001b[1;32m     74\u001b[0m prism_content \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mendmodule\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Write the content to the output file\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     78\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(prism_content)\n",
      "File \u001b[0;32m~/anaconda3/envs/luna/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'eval/prism/truthful_qa_alpaca_7B_with_semantics_HMM_400_500.pm'"
     ]
    }
   ],
   "source": [
    "semantic_dataset_dict = {\n",
    "    \"truthful_qa\": \"truth_probability\",\n",
    "    \"sst2\": \"is_ood\",\n",
    "    \"advglue++\": \"is_adversarial\",\n",
    "}\n",
    "\n",
    "\n",
    "def dtmc_to_prism_updated(\n",
    "    dtmc_dict,\n",
    "    semantic_value_dict,\n",
    "    output_file_path,\n",
    "    llm_name,\n",
    "    dataset,\n",
    "    semantic_dataset_dict,\n",
    "):\n",
    "    # Increment state numbers by 1 to free up state 0\n",
    "    incremented_dtmc_dict = {\n",
    "        start_state\n",
    "        + 1: {end_state + 1: prob for end_state, prob in transitions.items()}\n",
    "        for start_state, transitions in dtmc_dict.items()\n",
    "    }\n",
    "\n",
    "    semantic_value_dict = {\n",
    "        state + 1: semantic_value for state, semantic_value in semantic_value_dict.items()\n",
    "    }\n",
    "\n",
    "    # Find the maximum state number for the state range after incrementing\n",
    "    max_state = max(incremented_dtmc_dict.keys())\n",
    "\n",
    "    # Start writing the PRISM model file content\n",
    "    prism_content = f\"dtmc\\n\\nmodule {llm_name}\\n\\n\"\n",
    "\n",
    "    # Add the state declarations\n",
    "    prism_content += f\"// local state\\nstate : [0..{max_state}] init 0;\\n\"\n",
    "    prism_content += f\"{semantic_dataset_dict[dataset]} : [0..100] init 0;\\n\\n\"\n",
    "\n",
    "    # Add the initial transition from state 0 to state 1 with probability 1\n",
    "    prism_content += \"// Initial transition from state 0 to state 1\\n\"\n",
    "    prism_content += (\n",
    "        f\"[] state=0 -> 1 : (state'=1) & ({semantic_dataset_dict[dataset]}'={semantic_value_dict[1]});\\n\"\n",
    "    )\n",
    "\n",
    "    # Sort the states in ascending order and iterate over each start state in the incremented DTMC dictionary\n",
    "    for start_state in sorted(incremented_dtmc_dict.keys()):\n",
    "\n",
    "        transitions = incremented_dtmc_dict[start_state]\n",
    "        if start_state == 19:\n",
    "            print(transitions)\n",
    "\n",
    "        # Skip the initial state since it has been already handled\n",
    "        if start_state == 0:\n",
    "            continue\n",
    "\n",
    "        # Start the transitions for this state\n",
    "        transitions_str = (\n",
    "            f\"// Transitions from state {start_state}\\n[] state={start_state} -> \"\n",
    "        )\n",
    "\n",
    "        # Gather the transition probabilities and next states\n",
    "        transition_parts = []\n",
    "        for end_state, probability in sorted(transitions.items()):\n",
    "            # Format each transition part\n",
    "            transition_parts.append(\n",
    "                f\"{probability} : (state'={end_state}) & ({semantic_dataset_dict[dataset]}'={semantic_value_dict[end_state]})\"\n",
    "            )\n",
    "\n",
    "        # Concatenate transition parts with '+' and add to the transitions string\n",
    "        transitions_str += \" + \".join(transition_parts) + \";\\n\"\n",
    "\n",
    "        # Add the transitions to the PRISM model content\n",
    "        prism_content += transitions_str\n",
    "\n",
    "    # End the module\n",
    "    prism_content += \"\\nendmodule\\n\"\n",
    "\n",
    "    # Write the content to the output file\n",
    "    with open(output_file_path, \"w\") as f:\n",
    "        f.write(prism_content)\n",
    "\n",
    "\n",
    "# Specify the path for the updated output PRISM model file\n",
    "updated_output_prism_file_path = \"eval/prism/{}_{}_{}_{}_{}.pm\".format(\n",
    "    dataset, llm, model_type, abstract_state_num, pca_dim\n",
    ")\n",
    "\n",
    "# Convert the DTMC model dictionary to the updated PRISM format and write to a file\n",
    "if dataset == \"truthful_qa\":\n",
    "    train_state_matrix = prob_model.state_positive_prob_map\n",
    "    dtmc_to_prism_updated(\n",
    "        train_transition_matrix,\n",
    "        semantic_value_dict,\n",
    "        updated_output_prism_file_path,\n",
    "        llm,\n",
    "        dataset,\n",
    "        semantic_dataset_dict,\n",
    "    )\n",
    "else:\n",
    "    dtmc_to_prism_updated(\n",
    "        train_transition_matrix,\n",
    "        semantic_value_dict,\n",
    "        updated_output_prism_file_path,\n",
    "        llm,\n",
    "        dataset,\n",
    "        semantic_dataset_dict,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_abstract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
